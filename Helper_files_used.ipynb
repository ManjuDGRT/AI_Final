{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2Ng6JcjuwW01",
        "outputId": "6da48211-7778-4db6-8bd6-851a3ef92ae4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'deeplearning-models'...\n",
            "remote: Enumerating objects: 1463, done.\u001b[K\n",
            "remote: Counting objects: 100% (170/170), done.\u001b[K\n",
            "remote: Compressing objects: 100% (87/87), done.\u001b[K\n",
            "remote: Total 1463 (delta 87), reused 153 (delta 78), pack-reused 1293\u001b[K\n",
            "Receiving objects: 100% (1463/1463), 42.57 MiB | 23.68 MiB/s, done.\n",
            "Resolving deltas: 100% (717/717), done.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/rasbt/deeplearning-models\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "helper.py"
      ],
      "metadata": {
        "id": "cL9gABT3xH7k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import imageio\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "def quickdraw_npy_to_imagefile(inpath, outpath, filetype='png', subset=None):\n",
        "    \"\"\"\n",
        "    Creates a folder with subfolders for each image class\n",
        "    from the Quickdraw dataset (https://quickdraw.withgoogle.com)\n",
        "    downloaded in .npy format.\n",
        "\n",
        "    To download the .npy formatted dataset:\n",
        "      gsutil -m cp gs://quickdraw_dataset/full/numpy_bitmap/*.npy quickdraw-png\n",
        "\n",
        "    Usage example:\n",
        "      quickdraw_npy_to_imagefile('quickdraw-npy', 'quickdraw-png')\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "\n",
        "    inpath : str\n",
        "        string specifying the path to the input directory containing\n",
        "        the .npy files\n",
        "\n",
        "    outpath : str\n",
        "        string specifying the path for the output images\n",
        "\n",
        "    subset : tuple or list (default=None)\n",
        "        A subset of categories to consider. E.g.\n",
        "        `(\"lollipop\", \"binoculars\", \"mouse\", \"basket\")`\n",
        "\n",
        "    \"\"\"\n",
        "    if not os.path.exists(outpath):\n",
        "        os.mkdir(outpath)\n",
        "    npy_list = [i for i in os.listdir(inpath) if i.endswith('.npy')]\n",
        "\n",
        "    if subset:\n",
        "        npy_list = [i for i in npy_list if i.split('.npy')[0] in subset]\n",
        "\n",
        "    if not len(npy_list):\n",
        "        raise ValueError('No .npy files found in %s' % inpath)\n",
        "\n",
        "    npy_paths = [os.path.join(inpath, i) for i in npy_list]\n",
        "\n",
        "    for i, j in zip(npy_list, npy_paths):\n",
        "\n",
        "        label = (i.split('-')[-1]).split('.npy')[0]\n",
        "        folder = os.path.join(outpath, label)\n",
        "        if not os.path.exists(folder):\n",
        "            os.mkdir(folder)\n",
        "        X = np.load(j)\n",
        "\n",
        "        cnt = 0\n",
        "        for row in X:\n",
        "            img_array = row.reshape(28, 28)\n",
        "            assert cnt < 1000000\n",
        "            outfile = os.path.join(folder, '%s_%06d.%s' % (\n",
        "                label, cnt, filetype))\n",
        "            imageio.imwrite(outfile,\n",
        "                            img_array[:, :])\n",
        "            cnt += 1\n"
      ],
      "metadata": {
        "id": "1j0CRnj2wfRF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "helper_data.py"
      ],
      "metadata": {
        "id": "wXgA91HRxOIM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.utils.data import sampler\n",
        "from torchvision import datasets\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.utils.data import SubsetRandomSampler\n",
        "from torchvision import transforms\n",
        "\n",
        "\n",
        "class UnNormalize(object):\n",
        "    def __init__(self, mean, std):\n",
        "        self.mean = mean\n",
        "        self.std = std\n",
        "\n",
        "    def __call__(self, tensor):\n",
        "        \"\"\"\n",
        "        Parameters:\n",
        "        ------------\n",
        "        tensor (Tensor): Tensor image of size (C, H, W) to be normalized.\n",
        "\n",
        "        Returns:\n",
        "        ------------\n",
        "        Tensor: Normalized image.\n",
        "        \"\"\"\n",
        "        for t, m, s in zip(tensor, self.mean, self.std):\n",
        "            t.mul_(s).add_(m)\n",
        "        return tensor\n",
        "\n",
        "\n",
        "def get_dataloaders_mnist(batch_size, num_workers=0,\n",
        "                          validation_fraction=None,\n",
        "                          train_transforms=None,\n",
        "                          test_transforms=None):\n",
        "\n",
        "    if train_transforms is None:\n",
        "        train_transforms = transforms.ToTensor()\n",
        "\n",
        "    if test_transforms is None:\n",
        "        test_transforms = transforms.ToTensor()\n",
        "\n",
        "    train_dataset = datasets.MNIST(root='data',\n",
        "                                   train=True,\n",
        "                                   transform=train_transforms,\n",
        "                                   download=True)\n",
        "\n",
        "    valid_dataset = datasets.MNIST(root='data',\n",
        "                                   train=True,\n",
        "                                   transform=test_transforms)\n",
        "\n",
        "    test_dataset = datasets.MNIST(root='data',\n",
        "                                  train=False,\n",
        "                                  transform=test_transforms)\n",
        "\n",
        "    if validation_fraction is not None:\n",
        "        num = int(validation_fraction * 60000)\n",
        "        train_indices = torch.arange(0, 60000 - num)\n",
        "        valid_indices = torch.arange(60000 - num, 60000)\n",
        "\n",
        "        train_sampler = SubsetRandomSampler(train_indices)\n",
        "        valid_sampler = SubsetRandomSampler(valid_indices)\n",
        "\n",
        "        valid_loader = DataLoader(dataset=valid_dataset,\n",
        "                                  batch_size=batch_size,\n",
        "                                  num_workers=num_workers,\n",
        "                                  sampler=valid_sampler)\n",
        "\n",
        "        train_loader = DataLoader(dataset=train_dataset,\n",
        "                                  batch_size=batch_size,\n",
        "                                  num_workers=num_workers,\n",
        "                                  drop_last=True,\n",
        "                                  sampler=train_sampler)\n",
        "\n",
        "    else:\n",
        "        train_loader = DataLoader(dataset=train_dataset,\n",
        "                                  batch_size=batch_size,\n",
        "                                  num_workers=num_workers,\n",
        "                                  drop_last=True,\n",
        "                                  shuffle=True)\n",
        "\n",
        "    test_loader = DataLoader(dataset=test_dataset,\n",
        "                             batch_size=batch_size,\n",
        "                             num_workers=num_workers,\n",
        "                             shuffle=False)\n",
        "\n",
        "    if validation_fraction is None:\n",
        "        return train_loader, test_loader\n",
        "    else:\n",
        "        return train_loader, valid_loader, test_loader\n",
        "\n",
        "\n",
        "def get_dataloaders_cifar10(batch_size, num_workers=0,\n",
        "                            validation_fraction=None,\n",
        "                            train_transforms=None,\n",
        "                            test_transforms=None):\n",
        "\n",
        "    if train_transforms is None:\n",
        "        train_transforms = transforms.ToTensor()\n",
        "\n",
        "    if test_transforms is None:\n",
        "        test_transforms = transforms.ToTensor()\n",
        "\n",
        "    train_dataset = datasets.CIFAR10(root='data',\n",
        "                                     train=True,\n",
        "                                     transform=train_transforms,\n",
        "                                     download=True)\n",
        "\n",
        "    valid_dataset = datasets.CIFAR10(root='data',\n",
        "                                     train=True,\n",
        "                                     transform=test_transforms)\n",
        "\n",
        "    test_dataset = datasets.CIFAR10(root='data',\n",
        "                                    train=False,\n",
        "                                    transform=test_transforms)\n",
        "\n",
        "    if validation_fraction is not None:\n",
        "        num = int(validation_fraction * 50000)\n",
        "        train_indices = torch.arange(0, 50000 - num)\n",
        "        valid_indices = torch.arange(50000 - num, 50000)\n",
        "\n",
        "        train_sampler = SubsetRandomSampler(train_indices)\n",
        "        valid_sampler = SubsetRandomSampler(valid_indices)\n",
        "\n",
        "        valid_loader = DataLoader(dataset=valid_dataset,\n",
        "                                  batch_size=batch_size,\n",
        "                                  num_workers=num_workers,\n",
        "                                  sampler=valid_sampler)\n",
        "\n",
        "        train_loader = DataLoader(dataset=train_dataset,\n",
        "                                  batch_size=batch_size,\n",
        "                                  num_workers=num_workers,\n",
        "                                  drop_last=True,\n",
        "                                  sampler=train_sampler)\n",
        "\n",
        "    else:\n",
        "        train_loader = DataLoader(dataset=train_dataset,\n",
        "                                  batch_size=batch_size,\n",
        "                                  num_workers=num_workers,\n",
        "                                  drop_last=True,\n",
        "                                  shuffle=True)\n",
        "\n",
        "    test_loader = DataLoader(dataset=test_dataset,\n",
        "                             batch_size=batch_size,\n",
        "                             num_workers=num_workers,\n",
        "                             shuffle=False)\n",
        "\n",
        "    if validation_fraction is None:\n",
        "        return train_loader, test_loader\n",
        "    else:\n",
        "        return train_loader, valid_loader, test_loader\n",
        "\n"
      ],
      "metadata": {
        "id": "VMAMYFgtwiFo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "helper_evaluate.py"
      ],
      "metadata": {
        "id": "4Ew5EAUxxSPT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "from itertools import product\n",
        "\n",
        "\n",
        "def compute_accuracy(model, data_loader, device):\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        correct_pred, num_examples = 0, 0\n",
        "        for i, (features, targets) in enumerate(data_loader):\n",
        "\n",
        "            features = features.to(device)\n",
        "            targets = targets.to(device)\n",
        "\n",
        "            logits = model(features)\n",
        "            if isinstance(logits, torch.distributed.rpc.api.RRef):\n",
        "                logits = logits.local_value()\n",
        "            _, predicted_labels = torch.max(logits, 1)\n",
        "            num_examples += targets.size(0)\n",
        "            correct_pred += (predicted_labels == targets).sum()\n",
        "    return correct_pred.float()/num_examples * 100\n",
        "\n",
        "\n",
        "def compute_epoch_loss(model, data_loader, device):\n",
        "    model.eval()\n",
        "    curr_loss, num_examples = 0., 0\n",
        "    with torch.no_grad():\n",
        "        for features, targets in data_loader:\n",
        "            features = features.to(device)\n",
        "            targets = targets.to(device)\n",
        "            logits = model(features)\n",
        "            if isinstance(logits, torch.distributed.rpc.api.RRef):\n",
        "                logits = logits.local_value()\n",
        "            loss = F.cross_entropy(logits, targets, reduction='sum')\n",
        "            num_examples += targets.size(0)\n",
        "            curr_loss += loss\n",
        "\n",
        "        curr_loss = curr_loss / num_examples\n",
        "        return curr_loss\n",
        "\n",
        "\n",
        "def compute_confusion_matrix(model, data_loader, device):\n",
        "\n",
        "    all_targets, all_predictions = [], []\n",
        "    with torch.no_grad():\n",
        "\n",
        "        for i, (features, targets) in enumerate(data_loader):\n",
        "\n",
        "            features = features.to(device)\n",
        "            targets = targets\n",
        "            logits = model(features)\n",
        "            _, predicted_labels = torch.max(logits, 1)\n",
        "            all_targets.extend(targets.to('cpu'))\n",
        "            all_predictions.extend(predicted_labels.to('cpu'))\n",
        "\n",
        "    all_predictions = all_predictions\n",
        "    all_predictions = np.array(all_predictions)\n",
        "    all_targets = np.array(all_targets)\n",
        "\n",
        "    class_labels = np.unique(np.concatenate((all_targets, all_predictions)))\n",
        "    if class_labels.shape[0] == 1:\n",
        "        if class_labels[0] != 0:\n",
        "            class_labels = np.array([0, class_labels[0]])\n",
        "        else:\n",
        "            class_labels = np.array([class_labels[0], 1])\n",
        "    n_labels = class_labels.shape[0]\n",
        "    lst = []\n",
        "    z = list(zip(all_targets, all_predictions))\n",
        "    for combi in product(class_labels, repeat=2):\n",
        "        lst.append(z.count(combi))\n",
        "    mat = np.asarray(lst)[:, None].reshape(n_labels, n_labels)\n",
        "    return mat"
      ],
      "metadata": {
        "id": "TS6eFVQnwl2F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "helper_plotting.py"
      ],
      "metadata": {
        "id": "xDVV3L4DxV9H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# imports from installed libraries\n",
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import torch\n",
        "\n",
        "\n",
        "def plot_training_loss(minibatch_loss_list, num_epochs, iter_per_epoch,\n",
        "                       results_dir=None, averaging_iterations=100):\n",
        "\n",
        "    plt.figure()\n",
        "    ax1 = plt.subplot(1, 1, 1)\n",
        "    ax1.plot(range(len(minibatch_loss_list)),\n",
        "             (minibatch_loss_list), label='Minibatch Loss')\n",
        "\n",
        "    if len(minibatch_loss_list) > 1000:\n",
        "        ax1.set_ylim([\n",
        "            0, np.max(minibatch_loss_list[1000:])*1.5\n",
        "            ])\n",
        "    ax1.set_xlabel('Iterations')\n",
        "    ax1.set_ylabel('Loss')\n",
        "\n",
        "    ax1.plot(np.convolve(minibatch_loss_list,\n",
        "                         np.ones(averaging_iterations,)/averaging_iterations,\n",
        "                         mode='valid'),\n",
        "             label='Running Average')\n",
        "    ax1.legend()\n",
        "\n",
        "    ###################\n",
        "    # Set scond x-axis\n",
        "    ax2 = ax1.twiny()\n",
        "    newlabel = list(range(num_epochs+1))\n",
        "\n",
        "    newpos = [e*iter_per_epoch for e in newlabel]\n",
        "\n",
        "    ax2.set_xticks(newpos[::10])\n",
        "    ax2.set_xticklabels(newlabel[::10])\n",
        "\n",
        "    ax2.xaxis.set_ticks_position('bottom')\n",
        "    ax2.xaxis.set_label_position('bottom')\n",
        "    ax2.spines['bottom'].set_position(('outward', 45))\n",
        "    ax2.set_xlabel('Epochs')\n",
        "    ax2.set_xlim(ax1.get_xlim())\n",
        "    ###################\n",
        "\n",
        "    plt.tight_layout()\n",
        "\n",
        "    if results_dir is not None:\n",
        "        image_path = os.path.join(results_dir, 'plot_training_loss.pdf')\n",
        "        plt.savefig(image_path)\n",
        "\n",
        "\n",
        "def plot_accuracy(train_acc_list, valid_acc_list, results_dir):\n",
        "\n",
        "    num_epochs = len(train_acc_list)\n",
        "\n",
        "    plt.plot(np.arange(1, num_epochs+1),\n",
        "             train_acc_list, label='Training')\n",
        "    plt.plot(np.arange(1, num_epochs+1),\n",
        "             valid_acc_list, label='Validation')\n",
        "\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Accuracy')\n",
        "    plt.legend()\n",
        "\n",
        "    plt.tight_layout()\n",
        "\n",
        "    if results_dir is not None:\n",
        "        image_path = os.path.join(\n",
        "            results_dir, 'plot_acc_training_validation.pdf')\n",
        "        plt.savefig(image_path)\n",
        "\n",
        "\n",
        "def show_examples(model, data_loader, unnormalizer=None, class_dict=None):\n",
        "\n",
        "\n",
        "    for batch_idx, (features, targets) in enumerate(data_loader):\n",
        "\n",
        "        with torch.no_grad():\n",
        "            features = features\n",
        "            targets = targets\n",
        "            logits = model(features)\n",
        "            predictions = torch.argmax(logits, dim=1)\n",
        "        break\n",
        "\n",
        "    fig, axes = plt.subplots(nrows=3, ncols=5,\n",
        "                             sharex=True, sharey=True)\n",
        "\n",
        "    if unnormalizer is not None:\n",
        "        for idx in range(features.shape[0]):\n",
        "            features[idx] = unnormalizer(features[idx])\n",
        "    nhwc_img = np.transpose(features, axes=(0, 2, 3, 1))\n",
        "\n",
        "    if nhwc_img.shape[-1] == 1:\n",
        "        nhw_img = np.squeeze(nhwc_img.numpy(), axis=3)\n",
        "\n",
        "        for idx, ax in enumerate(axes.ravel()):\n",
        "            ax.imshow(nhw_img[idx], cmap='binary')\n",
        "            if class_dict is not None:\n",
        "                ax.title.set_text(f'P: {class_dict[predictions[idx].item()]}'\n",
        "                                  f'\\nT: {class_dict[targets[idx].item()]}')\n",
        "            else:\n",
        "                ax.title.set_text(f'P: {predictions[idx]} | T: {targets[idx]}')\n",
        "            ax.axison = False\n",
        "\n",
        "    else:\n",
        "\n",
        "        for idx, ax in enumerate(axes.ravel()):\n",
        "            ax.imshow(nhwc_img[idx])\n",
        "            if class_dict is not None:\n",
        "                ax.title.set_text(f'P: {class_dict[predictions[idx].item()]}'\n",
        "                                  f'\\nT: {class_dict[targets[idx].item()]}')\n",
        "            else:\n",
        "                ax.title.set_text(f'P: {predictions[idx]} | T: {targets[idx]}')\n",
        "            ax.axison = False\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "def plot_confusion_matrix(conf_mat,\n",
        "                          hide_spines=False,\n",
        "                          hide_ticks=False,\n",
        "                          figsize=None,\n",
        "                          cmap=None,\n",
        "                          colorbar=False,\n",
        "                          show_absolute=True,\n",
        "                          show_normed=False,\n",
        "                          class_names=None):\n",
        "\n",
        "    if not (show_absolute or show_normed):\n",
        "        raise AssertionError('Both show_absolute and show_normed are False')\n",
        "    if class_names is not None and len(class_names) != len(conf_mat):\n",
        "        raise AssertionError('len(class_names) should be equal to number of'\n",
        "                             'classes in the dataset')\n",
        "\n",
        "    total_samples = conf_mat.sum(axis=1)[:, np.newaxis]\n",
        "    normed_conf_mat = conf_mat.astype('float') / total_samples\n",
        "\n",
        "    fig, ax = plt.subplots(figsize=figsize)\n",
        "    ax.grid(False)\n",
        "    if cmap is None:\n",
        "        cmap = plt.cm.Blues\n",
        "\n",
        "    if figsize is None:\n",
        "        figsize = (len(conf_mat)*1.25, len(conf_mat)*1.25)\n",
        "\n",
        "    if show_normed:\n",
        "        matshow = ax.matshow(normed_conf_mat, cmap=cmap)\n",
        "    else:\n",
        "        matshow = ax.matshow(conf_mat, cmap=cmap)\n",
        "\n",
        "    if colorbar:\n",
        "        fig.colorbar(matshow)\n",
        "\n",
        "    for i in range(conf_mat.shape[0]):\n",
        "        for j in range(conf_mat.shape[1]):\n",
        "            cell_text = \"\"\n",
        "            if show_absolute:\n",
        "                num = conf_mat[i, j].astype(np.int64)\n",
        "                cell_text += format(num, 'd')\n",
        "                if show_normed:\n",
        "                    cell_text += \"\\n\" + '('\n",
        "                    cell_text += format(normed_conf_mat[i, j], '.2f') + ')'\n",
        "            else:\n",
        "                cell_text += format(normed_conf_mat[i, j], '.2f')\n",
        "            ax.text(x=j,\n",
        "                    y=i,\n",
        "                    s=cell_text,\n",
        "                    va='center',\n",
        "                    ha='center',\n",
        "                    color=\"white\" if normed_conf_mat[i, j] > 0.5 else \"black\")\n",
        "\n",
        "    if class_names is not None:\n",
        "        tick_marks = np.arange(len(class_names))\n",
        "        plt.xticks(tick_marks, class_names, rotation=90)\n",
        "        plt.yticks(tick_marks, class_names)\n",
        "\n",
        "    if hide_spines:\n",
        "        ax.spines['right'].set_visible(False)\n",
        "        ax.spines['top'].set_visible(False)\n",
        "        ax.spines['left'].set_visible(False)\n",
        "        ax.spines['bottom'].set_visible(False)\n",
        "    ax.yaxis.set_ticks_position('left')\n",
        "    ax.xaxis.set_ticks_position('bottom')\n",
        "    if hide_ticks:\n",
        "        ax.axes.get_yaxis().set_ticks([])\n",
        "        ax.axes.get_xaxis().set_ticks([])\n",
        "\n",
        "    plt.xlabel('predicted label')\n",
        "    plt.ylabel('true label')\n",
        "    return fig, ax"
      ],
      "metadata": {
        "id": "s493ubv2wqrE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "helper_train.py"
      ],
      "metadata": {
        "id": "e6uqKscZxcFQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "from itertools import product\n",
        "\n",
        "\n",
        "def compute_accuracy(model, data_loader, device):\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        correct_pred, num_examples = 0, 0\n",
        "        for i, (features, targets) in enumerate(data_loader):\n",
        "\n",
        "            features = features.to(device)\n",
        "            targets = targets.to(device)\n",
        "\n",
        "            logits = model(features)\n",
        "            if isinstance(logits, torch.distributed.rpc.api.RRef):\n",
        "                logits = logits.local_value()\n",
        "            _, predicted_labels = torch.max(logits, 1)\n",
        "            num_examples += targets.size(0)\n",
        "            correct_pred += (predicted_labels == targets).sum()\n",
        "    return correct_pred.float()/num_examples * 100\n",
        "\n",
        "\n",
        "def compute_epoch_loss(model, data_loader, device):\n",
        "    model.eval()\n",
        "    curr_loss, num_examples = 0., 0\n",
        "    with torch.no_grad():\n",
        "        for features, targets in data_loader:\n",
        "            features = features.to(device)\n",
        "            targets = targets.to(device)\n",
        "            logits = model(features)\n",
        "            if isinstance(logits, torch.distributed.rpc.api.RRef):\n",
        "                logits = logits.local_value()\n",
        "            loss = F.cross_entropy(logits, targets, reduction='sum')\n",
        "            num_examples += targets.size(0)\n",
        "            curr_loss += loss\n",
        "\n",
        "        curr_loss = curr_loss / num_examples\n",
        "        return curr_loss\n",
        "\n",
        "\n",
        "def compute_confusion_matrix(model, data_loader, device):\n",
        "\n",
        "    all_targets, all_predictions = [], []\n",
        "    with torch.no_grad():\n",
        "\n",
        "        for i, (features, targets) in enumerate(data_loader):\n",
        "\n",
        "            features = features.to(device)\n",
        "            targets = targets\n",
        "            logits = model(features)\n",
        "            _, predicted_labels = torch.max(logits, 1)\n",
        "            all_targets.extend(targets.to('cpu'))\n",
        "            all_predictions.extend(predicted_labels.to('cpu'))\n",
        "\n",
        "    all_predictions = all_predictions\n",
        "    all_predictions = np.array(all_predictions)\n",
        "    all_targets = np.array(all_targets)\n",
        "\n",
        "    class_labels = np.unique(np.concatenate((all_targets, all_predictions)))\n",
        "    if class_labels.shape[0] == 1:\n",
        "        if class_labels[0] != 0:\n",
        "            class_labels = np.array([0, class_labels[0]])\n",
        "        else:\n",
        "            class_labels = np.array([class_labels[0], 1])\n",
        "    n_labels = class_labels.shape[0]\n",
        "    lst = []\n",
        "    z = list(zip(all_targets, all_predictions))\n",
        "    for combi in product(class_labels, repeat=2):\n",
        "        lst.append(z.count(combi))\n",
        "    mat = np.asarray(lst)[:, None].reshape(n_labels, n_labels)\n",
        "    return mat"
      ],
      "metadata": {
        "id": "Hv20zbCTw-_o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "helper_utils.py"
      ],
      "metadata": {
        "id": "-mXKcA01xe0V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import random\n",
        "import numpy as np\n",
        "import torch\n",
        "\n",
        "\n",
        "def set_all_seeds(seed):\n",
        "    os.environ[\"PL_GLOBAL_SEED\"] = str(seed)\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "\n",
        "\n",
        "def set_deterministic():\n",
        "    if torch.cuda.is_available():\n",
        "        torch.backends.cudnn.benchmark = False\n",
        "        torch.backends.cudnn.deterministic = True\n",
        "    torch.set_deterministic(True)\n"
      ],
      "metadata": {
        "id": "k9uBTir4wzzk"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}